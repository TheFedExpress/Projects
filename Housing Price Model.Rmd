---
title: "Housing Price Model"
author: "Peter Goodridge"
date: "December 9, 2017"
output: 
  html_document:
    toc: yes
    toc_float: yes
    df_print: paged
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)
```


###Introduction

For this study, I am trying to determine which factors influence the median home value of a tract of land, and whether median value can be predicted.  This could help prospective home buyers narrow their search by finding undervalued areas that fulfill their buying requirements.  This data has great potential from a business standpoint.  It could be used by a real estate developer looking to buy into undervalued areas, or determine which of their assets are overvalued.  We will be evaluating the viability of such a model.

###About the Data 

The data was obtained from http://biostat.mc.vanderbilt.edu/DataSets

The data was originally used for the paper "Hedonic Housing Prices and the Demand for Clean Air", which is over 40 years old.  It is somewhat unclear how the data was collected, but the authors cite several people who contributed to the data collection.  It originated from many sources, such as city records and environmental research.  Each observation indicates a tract of land in the Boston area and there are 506 total observations.  Every variable in the dataset will be considered, but several are of particular interest.

Variables of interest:

- **Numer of Rooms:** Numerical <br>
- **Distance:** Numerical <br>
- **River:** Categorical <br>
- **Crime:** Numerical <br>

Variable descriptions from the dataset provider:

 **CRIME:**     per capita crime rate by town<br>
 **RIVER:**    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br>
 **NOX:**      nitric oxides concentration (parts per 10 million)<br>
 **RM:**       average number of rooms per dwelling<br>
 **AGE:**      proportion of owner-occupied units built prior to 1940<br>
 **DIS:**      weighted distances to five Boston employment centres<br>
 **RAD:**      index of accessibility to radial highways<br>
 **TAX:**      full-value property-tax rate per $10,000<br>
 **PTRATIO:**  pupil-teacher ratio by town<br>
 **LSTAT:**    % lower status of the population<br>
 **MEDV:**     Median value of owner-occupied homes in $1000's<br>


Though the data is quite old, results from this analysis will still be valuable.  Many of the features haven't changed much or at all, such as the size of houses, and the relative location of cities.  It still wouldn't be prudent to use the model to predict housing prices today, but it can inform us of the important variables, and whether a model using current data would be viable.  This was an observational study because there was no randomization, and no splitting of the observations into treatment/control groups.  The data is not independent because every observation shares something in common, metropolitan area.  Every observation is within commuting distance of Boston and has a high population density.  As such, it cannot be generalized to other parts of the state, and especially not other states.  

Even though some of these variables might be good predictors of value, a causal link cannot be established.  To establish causation, there must be treatment/control groups.  There is an intuitive reason for this.  Predictors like rooms and crime rate are indirectly measuring neighborhood affluence.  A neighborhood full of large, but run down homes near a polluted factory would not have higher home values than a neighborhood of medium sized, but new and well-maintained homes near a top school system.  Even though the neighborhood with more rooms, will generally have the higher value, that is not the case here.  Affluence is the driver and the other variables are just correlated with it.

###Exploring the Data

This dataset has a fair amount of variables, so we will start by getting an overall sense of it, allowing us to narrow the focus.

```{r}
library(Hmisc)
library(corrplot)
library(nlme)
library(tidyverse)
library(car)
library(pander)
getHdata(boston)
boston_subset <- boston %>%
  select(-(1:2), -cmedv, -black)
pander(summary(boston_subset))
```

Home values are capped at $50,000.  Crime jumps out as not having a symmetric distribution, with a mean around 3 and a max of over 80.  

```{r}

mat <- cor(select(boston_subset, -river))
```

**Correlation Plot (Darker Colors Indicate Correlation)**
```{r}
corrplot(mat, type = "upper")
```

Rooms and lstat have the strongest correlation with value.  Crime has a somewhat strong correlation, but distance's may not be a strong predictor variable.  This is possibly because of affluent suburbs somewhat far from Boston.  Many of the possible predictor variables appear to be related, which concerns us for a couple reasons.  First, one of the measures that informs us whether the variable offers utility, the p value, could be compromised.  Second, if we add additional data or remove variables, it could significantly change our model.  We will attempt to remedy this by including as few variables as possible in the final model.


<h4>**Value**</h4>

```{r}
describe(boston$value)
boston_subset %>% ggplot() + geom_histogram(aes(x = value), color = "black", fill = "darkblue", binwidth = 1)
```

Home value is somewhat right skewed, with a fairly long tail, which means that predictions at the extremes could suffer.  

<h4>**Rooms**</h4>

```{r}
describe(boston_subset$rooms)
boston_subset %>% ggplot() + geom_histogram(aes(x = rooms), color = "black", fill = "darkgreen") + labs(title = "Distribution of Rooms")
boston_subset %>% ggplot() + geom_point(aes(x = rooms, y = value)) + labs(title = "Rooms vs Value")
```

This will likely be the best predictor variable.  It has a good linear relationship with home value, though that relationship deteriorates somewhat at the extremes.

<h4>**Distance**</h4>

```{r}
describe(boston_subset$distance)
boston_subset %>% ggplot() + geom_histogram(aes(x = distance), color = "black", fill = "purple") + labs(title = "Distribution of Distance")

boston_subset$distance_ln <- log(boston$distance)
boston_subset %>% ggplot() + geom_histogram(aes(x = distance_ln), color = "black", fill = "purple") + labs(title = "Distribution of Log Distance")
boston_subset %>% ggplot() + geom_point(aes(x = distance_ln, y = value)) + labs(title = "Distance vs Value")
```

Performing the "log transform" by taking the natural logarithm of distance has made the distribution more symmetric. This is very desirable  There is a noticeable relationship, especially after the transform.

<h4>**Lstat**</h4>

```{r}
describe(boston_subset$lstat)
boston_subset %>% ggplot() + geom_histogram(aes(x = lstat), color = "black", fill = "yellow") + labs(title = "Distribution of Lstat")
boston_subset$lstat_ln <- log(boston$lstat)
boston_subset %>% ggplot() + geom_histogram(aes(x = lstat_ln), color = "black", fill = "yellow") + labs(title = "Distribution of Log Lstat")
boston_subset %>% ggplot() + geom_point(aes(x = lstat_ln, y = value)) + labs(title = "Log Lstat vs Value")
```

Again, we have a skewed variable where the transformation helps.  This relationship is almost as strong as that of room count. There is additional indication here that our predictions at higher value levels will suffer.


<h4>**Tax**</h4>

```{r}
describe(boston_subset$tax)
boston_subset %>% ggplot() + geom_histogram(aes(x = tax), color = "black", fill = "brown") + labs(title = "Distribution of Tax")
boston_subset %>% ggplot() + geom_point(aes(x = tax, y = value)) + labs(title = "Tax vs Value")
```

Tax does not have many distinct values.  There is a point in the top right corner that will could influence the model's predictions, so this should be remembered.

<h4>**Crime**</h4>

```{r}
describe(boston_subset$crime)

boston_subset %>% ggplot() + geom_histogram(aes(x = crime), color = "black", fill = "orange") + labs(title = "Distribution of Crime")
boston_subset %>% ggplot() + geom_point(aes(x = crime, y = value)) + labs(title = "Crime vs Value")
```

Crime is extremely skewed.  It will have to be transformed, but I don't believe the log transform we previously used to be the most appropriate.  Instead, we will take the cube root of crime.

```{r}
boston_subset$crime_cbrt <- boston$crime^(1/3)

boston_subset %>%  ggplot() + geom_histogram(aes(x = crime_cbrt), color = "black", fill = "orange") + labs(title = "Distribution of Crime Transformed")
boston_subset %>% ggplot() + geom_point(aes(x = crime_cbrt, y = value)) + labs(title = "Crime Tranformed vs Value")
boston_transform <- select(boston_subset, -crime, -distance, -lstat)

```

Taking the cube root helped with the skew and gives us a more linear relationship, one of the properties the model must hold.

###Home Value Model

We will start with every variable and remove them based on the p value.  As mentioned earlier, because many of the variables are related, it is desirable to cut as many variables as possible, so we won't bother with recalculating after each variable is removed.

```{r}
full_model <- lm(value ~ ., data = boston_transform)
pander(summary(full_model))
```

It looks like industrial, residential, longitude, latitude, and older can be removed.  These aren't necessarily bad predictors, but because they are correlated with other predictors, they don't add much to the model.  For instance, take the one predictor model with older:

<h4>**Using just "older"**</h4>

```{r}
older <- lm(value ~ older, boston)
pander(summary(older))
```

This variable is easily a significant predictor when taken alone.

<h4>**Second Model**</h4>

```{r}
m2 <- lm(value ~ ., data = select(boston_transform, -older, - industrial, - latitude, - longitude, -residential))
pander(summary(m2))
```

```{r}
boston_final <- boston_transform %>%
  select(-older, - industrial, - latitude, - longitude, -residential)
mat2 <- cor(select(boston_final, -river))
```

The second model looks quite good.  We have eliminated a few variables, without affecting the measure of the quality of prediction, R squared.  The "t value" can give us a rough rank of the best predictors.  The further from 0, the better, so the top predictors are transformed lstat, transformed distance, ptratio, and rooms.

**Correlation Plot (Darker Colors Indicate Correlation)**
```{r}
corrplot(mat2, type = "upper")
```

Based on the correlation plot, relation among predictors isn't as much of a problem as before.

###Checking model assumptions

We will next determine whether the model meets the assumptions the assumptions required to trust the results.

```{r}
diagnostics <- data.frame(m2$model, residuals = m2$residuals, fitted_values = m2$fitted.values, residuals_abs = abs(m2$residuals))
ggplot(diagnostics) + geom_point(aes(x = fitted_values, y = residuals)) + geom_hline(yintercept = 0)
qqnorm(diagnostics$residuals)
qqline(diagnostics$residuals)
ggplot(diagnostics) + geom_histogram(aes(x = residuals), color = "black", fill = "yellow")

ggplot(diagnostics) + geom_point(aes(x = fitted_values, y = residuals_abs))
ggplot(diagnostics) + geom_boxplot(aes(y = residuals, x = river)) 
resid_df <- data.frame(residuals = m2$residuals,town = boston$town)
test_model <- lm(residuals ~ town, resid_df)
summary(test_model)$fstatistic
```

1. We have already covered the first assumption, a linear relationship between the predictors and value with the exploratory analysis.

2. The plots of the residuals, the difference between the predicted and actual value, should be follow a symmetric distribution.  This isn't a major problem because the overall model and variable estimates are easily significant.  

3. The residual plot should also exhibit constant spread, which it does not.  At predicted values above 30, there aren't as many points, but the difference between the highest and lowest points increases.  This means, as we suspected earlier, that the model will become more inefficient at predicting higher home values.  

4. Lastly, the residuals shouldn't be related.  The most likely relation would be residuals from the same town, so it was tested whether town was a good predictor for residual size.  Town did turn out to be a significant predictor of residual, so this assumption was violated.  However, this assumption is more important for data with a time related element, e.g. stock prices.  In our case, we simply missed out on a possible predictor variable.  We will still not include town in the model because it would add too many variables, with each town becoming a variable


There are some minor shortcomings, but nothing invalidates our results.  The assumption we care most about, in this case, is the second, so we will attempt a couple remedial measures.


```{r}
gamma <- glm(value ~ ., family = "Gamma",data = select(boston_transform, -older, - industrial, - latitude, - longitude, -residential))

diagnostics_w <-data.frame(residuals = gamma$residuals, fitted_values = gamma$fitted)
ggplot(diagnostics_w) + geom_point(aes(x = fitted_values, y = residuals)) + geom_hline(yintercept = 0)

boston_log <- boston_transform
boston_log$log_value <- log(boston_subset$value)

model_log <- lm(log_value ~ ., data = select(boston_log, -value, -older, - industrial, - latitude, - longitude, -residential))
diagnostics_log <- data.frame(model_log$model, residuals = model_log$residuals, fitted_values = model_log$fitted.values, residuals_abs = abs(model_log$residuals))
ggplot(diagnostics_log) + geom_point(aes(x = fitted_values, y = residuals)) + geom_hline(yintercept = 0)
qqnorm(diagnostics_log$residuals)
qqline(diagnostics_log$residuals)
ggplot(diagnostics_log) + geom_histogram(aes(x = residuals), color = "black", fill = "lightblue")
```

The first new model does appear to decrease the spread of residuals.  With this model, the spread doesn't need to be constant, so the fact that they "fan" to some degree isn't a problem.  However, this model is more difficult to interpret, so it will be discarded.  The second model doesn't look any better than the original, as the spread, instead, increases at lower values.  

###Conclusion

For completeness' sake, the final model was:<br>
$Value=65.96+2.38*River-18.34*Nox+2.63*Rooms+.39*Highway-.0129*Tax-.8774*Ptratio-6.98*\ln { Distance } -8.87*\ln { Lstat } -2.89*\sqrt [ 3 ]{ Crime }$

However, the purpose of this analysis was to determine the quality and feasibility of a housing price model.  We will, therefore, turn our attention to evaluation and possible improvements.


Within this dataset, the predictor variables did an overall excellent job of predicting the response variable, value.  They explained four fifths of the variance in value.  The model only faltered somewhat at the extremes.  The best predictors can be classified in two groups, measures of neighborhood affluence, e.g. crime rate, and measures of the neighborhood's closeness to Boston, e.g. distance.  

The fact that residuals were not independent of town indicates that there is possibly something not easily measurable that contributes to property values.  Perhaps it is the resident's wealth, like was just mentioned, or perhaps it is something different:  a characteristic that makes certain towns unique and enjoyable places to live.

If we wanted to create a current and more generalizable dataset, we would start by sampling from various areas around the country.  Population density would be a good variable to add, as would an indication of whether the town is rural, suburban, or urban.
